# 数据处理

主要用于记录数据处理的相关笔记

## HSV模型简介以及利用HSV模型随机增强图像

### 图像HSV模型简介

HSV(Hue, Saturation, Value)是根据颜色的直观特性创建的一种颜色空间, 也称为六角锥体模型(Hexcone Model). 在HSV模型中, 颜色是由色度(Hue), 饱和度(Saturation), 明度(Value)共同组成

![HSV_model](E:\kuisu\typora\深度学习资料\数据处理\数据处理.assets\HSV_model.jpg)

如图所示, HSV模型中

- 色度(Hue), 使用角度度量的, 范围是从0°到360°(逆时针), 比如0°/360°代表红色, 120°代表原绿色, 240°代表蓝色

  ![Hue](E:\kuisu\typora\深度学习资料\数据处理\数据处理.assets\Hue.png)

- 饱和度(Saturation)表示颜色接近光谱色的程度. 一种颜色, 可以看成是某种光谱色与白色混合的结果. 其中光谱色所占的比例越大, 颜色越接近光谱色的程度就越高, 颜色的饱和度也就越高, 其范围是0-1.

  ![Saturation](E:\kuisu\typora\深度学习资料\数据处理\数据处理.assets\Saturation.png)

- 明度(Value)颜色明度的程度, 对于光源色, 明度值与发光体的亮度有关. 其范围是0(暗)到1(明)

  ![Value](E:\kuisu\typora\深度学习资料\数据处理\数据处理.assets\Value.png)

### RGB模型转HSV模型

参考[opencv官方文档](https://docs.opencv.org/master/de/d25/imgproc_color_conversions.html#color_convert_rgb_hsv):

首先将R,G,B分量数值缩放到范围0到1之间, 即除以255. 接下来按如下公式进行转换即可.
$$
\begin{gathered}
V=\max (R, G, B) \\
S= \begin{cases}\frac{V-\min (R, G, B)}{V} & \text { if } V \neq 0 \\
0 & \text { otherwise }\end{cases} \\
H= \begin{cases}60(G-B) /(V-\min (R, G, B)) & \text { if } \mathrm{V}=\mathrm{R} \\
120+60(B-R) /(V-\min (R, G, B)) & \text { if } \mathrm{V}=\mathrm{G} \\
240+60(R-B) /(V-\min (R, G, B)) & \text { if } \mathrm{V}=\mathrm{B} \\
0 & \text { if } \mathrm{R}=\mathrm{G}=\mathrm{B}\end{cases}
\end{gathered}
$$
转换之后V和S都是在0-1之间, H是在0°-360°之间(计算的结果可能小于0, 如果小于0就加上360)

假设将要将像素(110,20,50)分别对应RGB分量, 转换成HSV模型空间中
$$
(110,20,50)/255=(0.4314,0.0784,0.1961)\\
V=max(0.4314,0.0784,0.1961)=0.4314\\
S=\frac{0.4314-min(0.4314,0.0784,0.1961)}{0.4314}=0.8183\\
H=\frac{60(0.0784-0.1961)}{0.4314-min(0.4314,0.0784,0.1961)}=-20
$$
那么转换后的$V=0.4314,S=0.8183,$由于H小于0所以加上360即$H=340$

### opencv关于HSV模型实验

使用opencv将RGB模型图像转成HSV模型图像非常简单, 直接使用`cv2.cvtColor()`, 在code参数中传入`cv2.COLOR_RGB2HSV`参数即可. **但需注意, 通过opencv转HSV后会根据传入的数据类型缩放到不同范围, 如果输入的是Uin8类型的数据(一般读入图片数据类型都是Uint8), 默认缩放到0-255之间. ** 那么对于饱和度和明度(默认0-1之间)而言直接乘以255然后取整即可. 对于色度(默认是在0-360之间), 由于超出了Uint8数据类型的范围, 所以官方存储时是直接除以2即缩放到0-180之间.

![opencv_HSV](E:\kuisu\typora\深度学习资料\数据处理\数据处理.assets\opencv_HSV.png)

那么对于刚刚讲的示例将$(110,20,50)$RGB模型空间转到HSV模型空间得到是$(340,0.8183,0.4314)$,按照刚刚将的opencv转后后应该是:
$$
H: 340/2=170\\
S: 0.8183*255=209\\
V: 0.4314*255=110
$$
使用opencv转换试一下看对不对:

```python
import cv2
import numpy as np

rgb = np.array([110, 20, 50], dtype=np.uint8).reshape((1, 1, 3))
hsv = cv2.cvtColor(rgb, cv2.COLOR_RGB2HSV)
print(hsv)
```

终端打印的结果是`[[[170 209 110]]]`

接下来, 使用opencv来固定色度, 饱和度, 明度其中两个变量, 渐变剩下一个变量来看看吓偶句. 

**固定sat(饱和度)和val(明度), 渐变hue(色度)**. 从左到右数值从0到180(对应hue中0°到260°)

```python
import cv2
import numpy as np


hue = np.tile(np.arange(0, 180, dtype=np.uint8).reshape((1, 180, 1)),
             (50, 1, 1))
sat= np.ones((50, 180, 1), dtype=np.uint8) * 255
val = np.ones((50, 180, 1), dtype=np.uint8) * 255
img_hsv = cv2.merge((hue, sat, val))
img = cv2.cvtColor(img_hsv, cv2.COLOR_HSV2BGR)
# img = cv2.resize(img, (720, 100))
cv2.imshow("img", img)
cv2.waitKey(0)
```

![Hue](E:\kuisu\typora\深度学习资料\数据处理\数据处理.assets\Hue.png)

**固定hue(色度)以及val(明度), 渐变sat(饱和度)**. 从左到右数值从0到255(对应sat中0到1, 饱和度越来越高)

```python
import cv2
import numpy as np

hue = np.zeros((100, 256, 1), dtype=np.uint8)
sat = np.tile(np.arange(0, 256, dtype=np.uint8).reshape((1, 256, 1)),
             (100, 1, 1))
val = np.ones((100, 256, 1), dtype=np.uint8) * 255
img_hsv = cv2.merge((hue, sat, val))
img = cv2.cvtColor(img_hsv, cv2.COLOR_HSV2BGR)
# img = cv2.resize(img, (720, 100))
cv2.imshow("img", img)
cv2.waitKey(0)
```

![Saturation](E:\kuisu\typora\深度学习资料\数据处理\数据处理.assets\Saturation.png)

**固定hue(色度)以及sat(饱和度), 渐变val(明度)**. 从左到右数值从0到255(对应val中0到1, 明度越来越高)

```python
import cv2
import numpy as np

hue = np.zeros((100, 256, 1), dtype=np.uint8)
sat = np.ones((100, 256, 1), dtype=np.uint8) * 255
val = np.tile(np.arange(0, 256, dtype=np.uint8).reshape((1, 256, 1)),
             (100, 1, 1))
img_hsv = cv2.merge((hue, sat, val))
img = cv2.cvtColor(img_hsv, cv2.COLOR_HSV2BGR)
# img = cv2.resize(img, (720, 100))
cv2.imshow("img", img)
cv2.waitKey(0)
```

![Value](E:\kuisu\typora\深度学习资料\数据处理\数据处理.assets\Value.png)

### 随机增加图像HSV

下面的代码来自`yolov3 spp`项目(增强方法并不唯一), 这里结合以上讲的知识进行简单讲解

1. 首先传入图像`img`以及三个超参数`h_gain, s_gain, v_gain`
2. 使用`np.random.uniform`针对`h,s,v`分别生成了一个`[-1,1]`之间的随机数, 然后分别乘上传入的`h_gain, s_gain, v_gain`, 最后加1. 假设`h_gain=0.5`, 那么会在[0.5,1.5]之间随机生成一个倍率因子, 后面会将所有`hue`数值乘上这个倍率. `s_gain, v_gain`同理不再赘述
3. 使用`cvs.cvtColor`函数将传入的图片由BGR格式转成HSV, 使用`cv2.split`函数将HSV分量分开, 并分别赋值给`hue, sat, val`
4. 分别针对`hue, sat, val`生成对应的Lock-Up Table (LUT)查找表(记录变换前后数值的对应表). 就是将0-255范围内所有的数值都乘以刚刚生成的随机倍率因子构建LUT, 后面针对每个元素直接查表无需再计算. 注意, hue范围是再0到180之间的, 所以有个取余的操作`%180`, sat和val范围是0到255之间, 所以使用`np.clip`防止越界.
5. 使用`cv2.LUT`方法利用刚刚针对`hue, sat, val`生成的Look-Up Table进行变换. 变换后使用`cv2.merge`方法再将`hue, sat, val`分量合并为hsv图像
6. 最后使用`cv2.cvtColor`再将hsv图像转换回bgr图像

```python
import cv2
import numpy as np

def augment_hsv(img, h_gain=0.5, s_gain=0.5, v_gain=0.5):
    r = np.random.uniform(-1, 1, 3) * [h_gain, s_gain, v_gain] + 1  # random gains
    hue, sat, val = cv2.split(cv2.cvtColor(img, cv2.COLOR_BGR2HSV))
    dtype = img.dtype  # uint8

    x = np.arange(0, 256, dtype=np.int16)
    lut_hue = ((x * r[0]) % 180).astype(dtype)
    lut_sat = np.clip(x * r[1], 0, 255).astype(dtype)
    lut_val = np.clip(x * r[2], 0, 255).astype(dtype)

    img_hsv = cv2.merge((cv2.LUT(hue, lut_hue), cv2.LUT(sat, lut_sat), cv2.LUT(val, lut_val))).astype(dtype)
    aug_img = cv2.cvtColor(img_hsv, cv2.COLOR_HSV2BGR)
    return aug_img
```

下图是调用`argment_hsv`随机增强前, 后的效果, 左图是随机增强前, 有图是随机增强后

![augment_hsv](E:\kuisu\typora\深度学习资料\数据处理\数据处理.assets\augment_hsv.png)

## 数据增广: 旋转, 缩放, 平移以及错切

在图像领域, 为了提升训练样本数量, 数据增广是非常常见的手段如

- 随机水平翻转
- 随机色调(H), 饱和度(S), 明度(V)调整
- 随机旋转, 缩放, 平移以及错切

### 仿射变换

Affine Transformation 其实是另外两种简单变换的叠加: 一个是线性变换, 一个是平移变换

仿射变换包括缩放(scale), 平移(transform), 旋转(rotate), 镜像变换(relection), 错切(shear ampping, 对象的倒影), 原来的直线经仿射变换后还是直线, 原来的平行线经仿射变换后还是平行线. 

仿射变换中集合的一些性质保持不变:

1. 凸性
2. 共线性: 若几个点变换前在一条线上, 则放射变换后仍在一条线上
3. 平行线: 若两条线变换前平行, 则变换后仍然平行
4. 共线比例不变性: 变换前一条线上两条线段的比例, 在变换后比例不变.



### 数学表达式

二维图像变换中, 一般表达式为
$$
\left[\begin{array}{l}
x^{\prime} \\
y^{\prime} \\
1
\end{array}\right]=\left[\begin{array}{ccc}
R_{00} & R_{01} & T_{x} \\
R_{10} & R_{11} & T_{y} \\
0 & 0 & 1
\end{array}\right]\left[\begin{array}{l}
x \\
y \\
1
\end{array}\right]
$$
可以视为线性变换R和平移变换T的叠加. $x,y$是变换前的坐标, $x',y'$是变换后的坐标. 



### 理解

![仿射变换-0](E:\kuisu\typora\深度学习资料\数据处理\数据处理.assets\仿射变换-0-16414522041131.jpg)

### 旋转,平移和缩放

在图像处理中的坐标系, 水平向右为x轴正方向, 竖直向下为y轴正方向

对于图像的旋转, 缩放, 平移都可以直接通过opencv提供的`getRotationMatrix2D`方法来求得仿射矩阵, 需要传入旋转中心`center`, 旋转角度`angle`(逆时针为正), 以及缩放因子`scale`. 假设以图片为中心, 顺时针旋转30度, (opencv里是以逆时针为正, 所以`angle=-30`), 并缩放0.5倍

```python
import cv2

img = cv2.imread("1.png")
h, w = img.shape[:2]
m = cv2.getRotationMatrix2D(center-(w//2, h//2),angle=-30,scale=0.5)
print(m)
```

得到的旋转矩阵参数如下

```python
[[0.433    -0.25     198.14]
 [0.25      0.433     16.25]]
```

接着使用opencv中的`cv2.warpAffine`的方法利用求得的仿射矩阵做仿射变换, 其中`src`为原图像, `M`为放射矩阵, `dsize`为输出图像的大小, `borderValue`为边界填充颜色(注意BGR顺序)

```python
import cv2

img = cv2.imread("1.png")
h, w = img.shape[0], img.shape[1]
m = cv2.getRotationMatrix2D(center=(w // 2, h // 2), angle=-30, scale=0.5)
r_img = cv2.warpAffine(src=img, M=m, dsize=(w, h), borderValue=(0, 0, 0))
cv2.imshow("origin", img)
cv2.imshow("rotation_scale_trans", r_img)
cv2.waitKey(0)
```

![放射变换_旋转_缩放](E:\kuisu\typora\深度学习资料\数据处理\数据处理.assets\放射变换_旋转_缩放-16414530939322.png)

接着结合上面仿射变换公式来讲.

上面的操作其实可以分解成三步, 第一步沿着坐标原点旋转, 第二步缩放图片, 第三步平移图片.

![放射变换_旋转_缩放1](E:\kuisu\typora\深度学习资料\数据处理\数据处理.assets\放射变换_旋转_缩放1.png)

对于第一步的原点旋转对应的放射矩阵为`cv2.getRotationMatrix2D(center=(0,0),angle=-30,scale=1.0)`
$$
\left[\begin{array}{ccc}
R_{00} & R_{01} & T_{x} \\
R_{10} & R_{11} & T_{y} \\
0 & 0 & 1
\end{array}\right]
$$
当然这里可以直接使用旋转矩阵模板来计算, 但需要注意的是模板里的旋转矩阵默认y轴是竖直向上的, 但图像处理中y轴是竖直向下的, 且都是以逆时针旋转为正, 所以这里的$\theta=30°$:
$$
\left[\begin{array}{ccc}
cos(\theta) & -sin(\theta) & 0 \\
sin(\theta) & cos{\theta} & 0 \\
0 & 0 & 1
\end{array}\right]
$$
对于第二部图片的缩放, 直接使用如下缩放矩阵:
$$
\left[\begin{array}{ccc}
S_{x} & 0 & 0 \\
0 & S_y & 0 \\
0 & 0 & 1
\end{array}\right]
$$
假设要将图片缩放0.5倍, 那么缩放矩阵为
$$
\left[\begin{array}{ccc}
0.5 & 0 & 0 \\
0 & 0.5 & 0 \\
0 & 0 & 1
\end{array}\right]
$$
及那个旋转和缩放结合起来就是(注意顺序), 由于矩阵乘法满足结合律, 所以可以将两个放射矩阵相乘
$$
\begin{gathered}
{\left[\begin{array}{l}
x^{\prime} \\
y^{\prime} \\
1
\end{array}\right]=\left[\begin{array}{lll}
0.5 & 0 & 0 \\
0 & 0.5 & 0 \\
0 & 0 & 1
\end{array}\right]\left(\left[\begin{array}{lll}
0.866 & -0.5 & 0 \\
0.5 & 0.866 & 0 \\
0 & 0 & 1
\end{array}\right]\left[\begin{array}{l}
x \\
y \\
1
\end{array}\right]\right)} \\
=\left[\begin{array}{lll}
0.433 & -0.25 & 0 \\
0.25 & 0.433 & 0 \\
0 & 0 & 1
\end{array}\right]\left[\begin{array}{l}
x \\
y \\
1
\end{array}\right]
\end{gathered}
$$

### 错切

图像的错切变换实际上是平面景物在投影平面上的非垂直投影效果. 图像变换也称为图像剪切, 错位或错移变换. 下图分别展示了沿x轴方向错切, y轴方向错切, 以及同时沿x, y轴两个方向错切的效果

![放射变换_错切](E:\kuisu\typora\深度学习资料\数据处理\数据处理.assets\放射变换_错切.png)

在opencv中并没有直接针对错切生成放射矩阵的方法, 所以我们自己可以构建错切对应的放射矩阵, 然后利用`cv2.warpAffine`进行放射变换即可. 虾米那是错切对应的放射矩阵, 其中$\theta$表示错切角度, $tan(\theta_1)$是x轴方向的错切参数, $tan(\theta_2)$是y轴方向的错切参数.
$$
\left[\begin{array}{l}
x^{\prime} \\
y^{\prime} \\
1
\end{array}\right]=\left[\begin{array}{ccc}
1 & tan(\theta_1) & T_{x} \\
tan(\theta_2) & 1 & T_{y} \\
0 & 0 & 1
\end{array}\right]\left[\begin{array}{l}
x \\
y \\
1
\end{array}\right]
$$

```python
import math
import cv2
import numpy as np


img = cv2.imread("1.png")
cv2.imshow("origin", img)
h, w = img.shape[0], img.shape[1]
origin_coord = np.array([[0, 0, 1], [w, 0, 1], [w, h, 1], [0, h, 1]])

theta = 30  # shear角度
tan = math.tan(math.radians(theta))

# x方向错切
m = np.eye(3)
m[0, 1] = tan
shear_coord = (m @ origin_coord.T).T.astype(np.int)
shear_img = cv2.warpAffine(src=img, M=m[:2],
                           dsize=(np.max(shear_coord[:, 0]), np.max(shear_coord[:, 1])),
                           borderValue=(0, 0, 0))
cv2.imshow("shear_x", shear_x)
cv2.waitKey(0)
```



## COCO数据集

COCO数据集的介绍
        COCO数据集是微软公司出资标注的数据集，主要用于目标检测、分割和图像描述。根据官网的介绍，它主要有以下的几种特性：

- Object Segmentation：目标分割
- Recognition in Context：图像情景识别
- Superpixel stuff segmentation：超像素分割
- 330K images(>200K labeled)：有330K张图片，其中超过200K张是标注过的
- 1.5 million object instances：150万个对象实例
- 80 object categories：80个目标类别
- 91 stuff categories：91个对象类别
- 5 captions per image：每张图片有5个描述
- 250K people with keypotins：有25万人的图片进行了关键点标注
   前几个特性都是很好理解也是属于比较热门的几个研究方向，主要的疑惑点是在 80 object categories 和 91 stuff categories ，接下来进行解释：

- 对于所谓的“stuff categories”，论文中的描述是where “stuff” categories include materials and objects with no clear boundaries (sky, street, grass)，即标注了91类没有明确边界的对象(诸如天空，街道，草地)。
- 其次注意80 object categories和91 stuff categories的区别，论文中用一段文字来描述了它们的区别，简单来说就是80类是91类的一个子集，去掉了一些难以分类和容易混淆的类别，一般来说我们都会使用这个80分类。
  80类别包含：



### COCO数据集标注格式

​        首先从coco官网下载数据集，本文以coco2017为例，下载得到train,val以及annotations，创建一个coco2017目录：

├── coco2017: 数据集根目录
     ├── train2017: 所有训练图像文件夹(118287张)
     ├── val2017: 所有验证图像文件夹(5000张)
     └── annotations: 对应标注文件夹
               ├── instances_train2017.json: 对应目标检测、分割任务的训练集标注文件
               ├── instances_val2017.json: 对应目标检测、分割任务的验证集标注文件
               ├── captions_train2017.json: 对应图像描述的训练集标注文件
               ├── captions_val2017.json: 对应图像描述的验证集标注文件
               ├── person_keypoints_train2017.json: 对应人体关键点检测的训练集标注文件
               └── person_keypoints_val2017.json: 对应人体关键点检测的验证集标注文件夹
          
其中annotation中，我仅关注instances_train2017.json和instances_val2017.json两个用于目标检测的标注文件。
    接下来我来分析标注文件信息中对我有用的信息，使用json库来查看标注文件，输入以下程序：

```python
import json
file_path = './instances_val2017.json'
json_info = json.load(open(file_path,'r'))
print(json_info["info"])
```

然后在第四行前插入断点，进行调试，在变量表可以看到以下信息：

![img](E:\kuisu\typora\深度学习资料\数据处理\数据处理.assets\watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAVGlhb18xMg==,size_20,color_FFFFFF,t_70,g_se,x_16)

### COCO函数接口

![img](E:\kuisu\typora\深度学习资料\数据处理\数据处理.assets\watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAVGlhb18xMg==,size_20,color_FFFFFF,t_70,g_se,x_16-16356776352755)

- getAnnIds()

  获取某图像的所有标注信息的idx列表

  ![img](E:\kuisu\typora\深度学习资料\数据处理\数据处理.assets\watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAVGlhb18xMg==,size_20,color_FFFFFF,t_70,g_se,x_16-16356776803707)

- loadAnns()

  通常配合getAnnIds()使用, 返回对应标注信息idex的标注详细信息

  ![img](E:\kuisu\typora\深度学习资料\数据处理\数据处理.assets\watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAVGlhb18xMg==,size_16,color_FFFFFF,t_70,g_se,x_16)

- 获取的target实例

  ![img](E:\kuisu\typora\深度学习资料\数据处理\数据处理.assets\watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAVGlhb18xMg==,size_20,color_FFFFFF,t_70,g_se,x_16-163567776441810)

- loadImgd()

  返回对应图片的详细信息

  ![img](E:\kuisu\typora\深度学习资料\数据处理\数据处理.assets\watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAVGlhb18xMg==,size_20,color_FFFFFF,t_70,g_se,x_16-163567780144912)

pycocotools简单使用实例
        

```python
import os
from pycocotools.coco import COCO
from PIL import Image, ImageDraw
import matplotlib.pyplot as plt

val_annotation_file = "./annotations/instances_val2017.json"
val_img_file = './val2017'

coco = COCO(annotation_file=val_annotation_file)
coco_classes = dict([(v["id"], v["name"]) for k, v in coco.cats.items()])

idx = list(sorted(coco.imgs.keys()))
img_id = idx[0] #排序后最小的图片id为139 ，即img_id=139

ann_idx = coco.getAnnIds(imgIds=img_id)
objects = coco.loadAnns(ann_idx)
#获取图片
##获取图片路径名
path = coco.loadImgs(img_id)[0]["file_name"]
##读取139号图片
img = Image.open(os.path.join(val_img_file, path)).convert('RGB')
#在图片上绘制矩形框
draw = ImageDraw.Draw(img)
##一个图片可能会含有多个锚框，对每一个都进行描绘
for object in objects:
    x,y,w,h = object["bbox"]
    x1,y1,x2,y2 = x, y, int(x+w), int(y+h)
    draw.rectangle((x1, y1, x2, y2))
    draw.text((x1, y1), coco_classes[object["category_id"]])
##使用matplotlib绘制
plt.imshow(img)
plt.show()
```

 最后得到结果如图所示：

![img](E:\kuisu\typora\深度学习资料\数据处理\数据处理.assets\watermark,type_ZHJvaWRzYW5zZmFsbGJhY2s,shadow_50,text_Q1NETiBAVGlhb18xMg==,size_20,color_FFFFFF,t_70,g_se,x_16-16356775876183)

## VOC数据

PASCAL VOC 挑战赛主要有 `Object Classification 、Object Detection、Object Segmentation、Human Layout、Action Classification` 这几类子任务

PASCAL VOC 2007 和 2012 数据集总共分 **4 个大类**：vehicle、household、animal、person，总共 **20 个小类（加背景 21 类）**，预测的时候是只输出下图中**黑色粗体的类别**
![在这里插入图片描述](E:\kuisu\typora\深度学习资料\数据处理\数据处理.assets\watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L216cG16aw==,size_16,color_FFFFFF,t_70)

### 组织结构

```python
.
├── Annotations 进行 detection 任务时的标签文件，xml 形式，文件名与图片名一一对应
├── ImageSets 包含三个子文件夹 Layout、Main、Segmentation，其中 Main 存放的是分类和检测的数据集分割文件
├── JPEGImages 存放 .jpg 格式的图片文件
├── SegmentationClass 存放按照 class 分割的图片
└── SegmentationObject 存放按照 object 分割的图片

├── Main
│   ├── train.txt 写着用于训练的图片名称， 共 2501 个
│   ├── val.txt 写着用于验证的图片名称，共 2510 个
│   ├── trainval.txt train与val的合集。共 5011 个
│   ├── test.txt 写着用于测试的图片名称，共 4952 个

```

### 标注标准

```python
<annotation>
	<folder>VOC2007</folder>
	<filename>000001.jpg</filename>  # 文件名 
	<source>
		<database>The VOC2007 Database</database>
		<annotation>PASCAL VOC2007</annotation>
		<image>flickr</image>
		<flickrid>341012865</flickrid>
	</source>
	<owner>
		<flickrid>Fried Camels</flickrid>
		<name>Jinky the Fruit Bat</name>
	</owner>
	<size>  # 图像尺寸, 用于对 bbox 左上和右下坐标点做归一化操作
		<width>353</width>
		<height>500</height>
		<depth>3</depth>
	</size>
	<segmented>0</segmented>  # 是否用于分割
	<object>
		<name>dog</name>  # 物体类别
		<pose>Left</pose>  # 拍摄角度：front, rear, left, right, unspecified 
		<truncated>1</truncated>  # 目标是否被截断（比如在图片之外），或者被遮挡（超过15%）
		<difficult>0</difficult>  # 检测难易程度，这个主要是根据目标的大小，光照变化，图片质量来判断
		<bndbox>
			<xmin>48</xmin>
			<ymin>240</ymin>
			<xmax>195</xmax>
			<ymax>371</ymax>
		</bndbox>
	</object>
	<object>
		<name>person</name>
		<pose>Left</pose>
		<truncated>1</truncated>
		<difficult>0</difficult>
		<bndbox>
			<xmin>8</xmin>
			<ymin>12</ymin>
			<xmax>352</xmax>
			<ymax>498</ymax>
		</bndbox>
	</object>
</annotation>

```



## labelme2labelImg

### create xml ann

```python

class CreateAnno:
    def __init__(self, ):
        self.doc = Document()  # 创建DOM文档对象
        self.anno = self.doc.createElement('annotation')  # 创建根元素
        self.doc.appendChild(self.anno)

        self.add_folder()
        self.add_path()
        self.add_source()
        self.add_segmented()

        # self.add_filename()
        # self.add_pic_size(width_text_str=str(width), height_text_str=str(height), depth_text_str=str(depth))

    def add_folder(self, floder_text_str='JPEGImages'):
        floder = self.doc.createElement('floder')  ##建立自己的开头
        floder_text = self.doc.createTextNode(floder_text_str)  ##建立自己的文本信息
        floder.appendChild(floder_text)  ##自己的内容
        self.anno.appendChild(floder)

    def add_filename(self, filename_text_str='00000.jpg'):
        filename = self.doc.createElement('filename')
        filename_text = self.doc.createTextNode(filename_text_str)
        filename.appendChild(filename_text)
        self.anno.appendChild(filename)

    def add_path(self, path_text_str="None"):
        path = self.doc.createElement('path')
        path_text = self.doc.createTextNode(path_text_str)
        path.appendChild(path_text)
        self.anno.appendChild(path)

    def add_source(self, database_text_str="Unknow"):
        source = self.doc.createElement('source')
        database = self.doc.createElement('database')
        database_text = self.doc.createTextNode(database_text_str)  # 元素内容写入
        database.appendChild(database_text)
        source.appendChild(database)
        self.anno.appendChild(source)

    def add_pic_size(self, width_text_str="0", height_text_str="0", depth_text_str="3"):
        size = self.doc.createElement('size')
        width = self.doc.createElement('width')
        width_text = self.doc.createTextNode(width_text_str)  # 元素内容写入
        width.appendChild(width_text)
        size.appendChild(width)

        height = self.doc.createElement('height')
        height_text = self.doc.createTextNode(height_text_str)
        height.appendChild(height_text)
        size.appendChild(height)

        depth = self.doc.createElement('depth')
        depth_text = self.doc.createTextNode(depth_text_str)
        depth.appendChild(depth_text)
        size.appendChild(depth)

        self.anno.appendChild(size)

    def add_segmented(self, segmented_text_str="0"):
        segmented = self.doc.createElement('segmented')
        segmented_text = self.doc.createTextNode(segmented_text_str)
        segmented.appendChild(segmented_text)
        self.anno.appendChild(segmented)

    def add_object(self,
                   name_text_str="None",
                   xmin_text_str="0",
                   ymin_text_str="0",
                   xmax_text_str="0",
                   ymax_text_str="0",
                   pose_text_str="Unspecified",
                   truncated_text_str="0",
                   difficult_text_str="0"):
        object = self.doc.createElement('object')
        name = self.doc.createElement('name')
        name_text = self.doc.createTextNode(name_text_str)
        name.appendChild(name_text)
        object.appendChild(name)

        pose = self.doc.createElement('pose')
        pose_text = self.doc.createTextNode(pose_text_str)
        pose.appendChild(pose_text)
        object.appendChild(pose)

        truncated = self.doc.createElement('truncated')
        truncated_text = self.doc.createTextNode(truncated_text_str)
        truncated.appendChild(truncated_text)
        object.appendChild(truncated)

        difficult = self.doc.createElement('Difficult')
        difficult_text = self.doc.createTextNode(difficult_text_str)
        difficult.appendChild(difficult_text)
        object.appendChild(difficult)

        bndbox = self.doc.createElement('bndbox')
        xmin = self.doc.createElement('xmin')
        xmin_text = self.doc.createTextNode(xmin_text_str)
        xmin.appendChild(xmin_text)
        bndbox.appendChild(xmin)

        ymin = self.doc.createElement('ymin')
        ymin_text = self.doc.createTextNode(ymin_text_str)
        ymin.appendChild(ymin_text)
        bndbox.appendChild(ymin)

        xmax = self.doc.createElement('xmax')
        xmax_text = self.doc.createTextNode(xmax_text_str)
        xmax.appendChild(xmax_text)
        bndbox.appendChild(xmax)

        ymax = self.doc.createElement('ymax')
        ymax_text = self.doc.createTextNode(ymax_text_str)
        ymax.appendChild(ymax_text)
        bndbox.appendChild(ymax)
        object.appendChild(bndbox)

        self.anno.appendChild(object)

    def get_anno(self):
        return self.anno

    def get_doc(self):
        return self.doc

    def save_doc(self, save_path):
        with open(save_path, "w") as f:
            self.doc.writexml(f, indent='\t', newl='\n', addindent='\t', encoding='utf-8')

```

### read json anno

```python

class ReadAnno:
    def __init__(self, json_path, process_mode="rectangle"):
        with open(json_path, 'r', encoding='gb18030') as fp:
            self.json_data = json.load(fp)
            # self.json_data = json.load(open(json_path))
        self.filename = self.json_data['imagePath']
        self.width = self.json_data['imageWidth']
        self.height = self.json_data['imageHeight']

        self.coordis = []
        assert process_mode in ["rectangle", "polygon"]
        if process_mode == "rectangle":
            self.process_polygon_shapes()
        elif process_mode == "polygon":
            self.process_polygon_shapes()

    def process_rectangle_shapes(self):
        for single_shape in self.json_data['shapes']:
            bbox_class = single_shape['label']
            xmin = single_shape['points'][0][0]
            ymin = single_shape['points'][0][1]
            xmax = single_shape['points'][1][0]
            ymax = single_shape['points'][1][1]
            self.coordis.append([xmin, ymin, xmax, ymax, bbox_class])

    def process_polygon_shapes(self):
        for single_shape in self.json_data['shapes']:
            bbox_class = single_shape['label']
            temp_points = []
            for couple_point in single_shape['points']:
                x = float(couple_point[0])
                y = float(couple_point[1])
                temp_points.append([x, y])
            temp_points = np.array(temp_points)
            xmin, ymin = temp_points.min(axis=0)
            xmax, ymax = temp_points.max(axis=0)
            self.coordis.append([xmin, ymin, xmax, ymax, bbox_class])

    def get_width_height(self):
        return self.width, self.height

    def get_filename(self):
        return self.filename

    def get_coordis(self):
        return self.coordis
```

### main

```python

from read_json_anno import ReadAnno
from create_xml_anno import CreateAnno


def json_transform_xml(json_path, xml_path, imagePath, process_mode="rectangle"):
    json_path = json_path
    print(json_path)
    json_anno = ReadAnno(json_path, process_mode=process_mode)
    width, height = json_anno.get_width_height()
    filename = json_anno.get_filename()
    coordis = json_anno.get_coordis()

    xml_anno = CreateAnno()
    xml_anno.add_filename(imagePath)
    xml_anno.add_pic_size(width_text_str=str(width), height_text_str=str(height), depth_text_str=str(3))
    for xmin, ymin, xmax, ymax, label in coordis:
        if ((xmax - xmin) < (width * 2 / 3)):
            # xml_anno.add_object(name_text_str=str("text"),
            xml_anno.add_object(name_text_str=str(label),
                                xmin_text_str=str(int(xmin)),
                                ymin_text_str=str(int(ymin)),
                                xmax_text_str=str(int(xmax)),
                                ymax_text_str=str(int(ymax)))

    xml_anno.save_doc(xml_path)


if __name__ == "__main__":
    root_json_dir = "../Segmentation"  # json文件夹路径
    root_save_xml_dir = "./xml"  # 转换后保存的xml文件夹路径

    for json_filename in tqdm(os.listdir(root_json_dir)):
        json_path = os.path.join(root_json_dir, json_filename)
        save_xml_path = os.path.join(root_save_xml_dir, json_filename.replace(".json", ".xml"))
        filepath, tmpfilename = os.path.split(json_filename)
        shotname, extension = os.path.splitext(tmpfilename)
        img_path = shotname + ".jpg"
        json_transform_xml(json_path, save_xml_path, img_path, process_mode="polygon")
```



### 将labelme标注框转为labelImg的标注框

实现将label的标注框转为labelImg的标注框格式(xml), 同时实现将标注框的label进行名称的更改.

```python
import argparse
import glob
import json
import os
import os.path as osp
import sys

import imgviz
import numpy as np
import PIL.Image

import labelme


colormap = [[i,i,i] for i in range(255)]
colormap.insert(0,[255,255,255])
colormap = np.array(colormap,dtype=np.uint8)
def lblsave(filename, lbl,colormap=colormap):

    # if osp.splitext(filename)[1] != ".png":
    #     filename += ".png"
    # Assume label ranses [-1, 254] for int32,
    # and [0, 255] for uint8 as VOC.
    if lbl.min() >= -1 and lbl.max() < 255:
        lbl_pil = PIL.Image.fromarray(lbl.astype(np.uint8), mode="P")

        lbl_pil.putpalette(colormap.flatten())
        lbl_pil.save(filename)
    else:
        raise ValueError(
            "[%s] Cannot save the pixel-wise class label as PNG. "
            "Please consider using the .npy format." % filename
        )

def editLabel(data):
    label = data['shape']['label']
    if label=='screen_normal':
        data['shape']['label']="screenN"
    elif label == 'screen_unnormal':
        data['shape']['label']='screenU'
    return data


def main():
    parser = argparse.ArgumentParser(
        formatter_class=argparse.ArgumentDefaultsHelpFormatter
    )
    parser.add_argument('input_dir', help='input annotated directory')
    parser.add_argument('output_dir', help='output dataset directory')
    parser.add_argument('--labels', help='labels file', required=True)
    parser.add_argument(
        '--noviz', help='no visualization', action='store_true'
    )
    args = parser.parse_args()

    if osp.exists(args.output_dir):
        print('Output directory already exists:', args.output_dir)
        sys.exit(1)
    os.makedirs(args.output_dir)
    os.makedirs(osp.join(args.output_dir, 'JPEGImages'))
    os.makedirs(osp.join(args.output_dir, 'SegmentationClassPNG'))
    os.makedirs(osp.join(args.output_dir, 'SegmentationInstancePNG'))
    if not args.noviz:
        os.makedirs(
            osp.join(args.output_dir, 'SegmentationClassVisualization')
        )
    print('Creating dataset:', args.output_dir)

    class_names = []
    class_name_to_id = {}
    for i, line in enumerate(open(args.labels).readlines()):
        class_id = i - 1  # starts with -1
        class_name = line.strip()
        class_name_to_id[class_name] = class_id
        if class_id == -1:
            assert class_name == '__ignore__'
            continue
        elif class_id == 0:
            assert class_name == '_background_'
        class_names.append(class_name)
    class_names = tuple(class_names)
    print('class_names:', class_names)
    out_class_names_file = osp.join(args.output_dir, 'class_names.txt')
    with open(out_class_names_file, 'w') as f:
        f.writelines('\n'.join(class_names))
    print('Saved class_names:', out_class_names_file)

    for label_file in glob.glob(osp.join(args.input_dir, '*.json')):
        print('Generating dataset from:', label_file)
        with open(label_file) as f:
            base = osp.splitext(osp.basename(label_file))[0]
            out_img_file = osp.join(
                args.output_dir, 'JPEGImages', base + '.jpg')
            out_lbl_file = osp.join(
                args.output_dir, 'SegmentationClass', base + '.npy')
            out_png_file = osp.join(
                args.output_dir, 'SegmentationClassPNG', base + '_t.bmp')
            out_instance_file = osp.join(
                args.output_dir, 'SegmentationInstancePNG', base + '_t.bmp')
            if not args.noviz:
                out_viz_file = osp.join(
                    args.output_dir,
                    'SegmentationClassVisualization',
                    base + '.jpg',
                )

            data = json.load(f)

            img_file = osp.join(osp.dirname(label_file), data['imagePath'])
            img = np.asarray(PIL.Image.open(img_file))
            PIL.Image.fromarray(img).save(out_img_file)

            cls, ins = labelme.utils.shapes_to_label(
                img_shape=img.shape,
                shapes=data['shapes'],
                label_name_to_value=class_name_to_id,
            )
            #修改调色版, 使其满足vision各mask格式
            lblsave(out_png_file, cls)
            lblsave(out_instance_file, ins)

            # np.save(out_lbl_file, lbl)

            if not args.noviz:
                viz = imgviz.label2rgb(
                    label=cls,
                    image=imgviz.rgb2gray(img),
                    font_size=15,
                    label_names=class_names,
                    loc='rb',
                )
                imgviz.io.imsave(out_viz_file, viz)


if __name__ == '__main__':
    main()

```



## vision数据处理

海康威视数据处理

生成海康威视的xml语义分割数据

```python
from xml.dom.minidom import Document

class CreateAnno:
    def __init__(self, ):
        self.doc = Document()  # 创建DOM文档对象
        self.anno = self.doc.createElement('VisionMaster.ModuleMainWindow.ModuleDialogNew.DeepLearning.FlawTrainData')  # 创建根元素
        self.doc.appendChild(self.anno)

        self.itemsData = self.doc.createElement("_ItemsData")
        self.anno.appendChild(self.itemsData)

        # self.add_object()
        # self.add_isOkCalibrated()
        # self.add_path()


        # self.add_filename()
        # self.add_pic_size(width_text_str=str(width), height_text_str=str(height), depth_text_str=str(depth))

    def add_itemsData(self):
        itemsData = self.doc.createElement("_ItemsData")
        self.anno.appendChild(itemsData)

    def add_object(self,
                   levelNum_str="None",
                   flags_str = '',
                   colorValue_str = '',
                   backgroundColor_str = '',
                   coord="",
                   sing_str='',
                   visible_str=''):


        object = self.doc.createElement('VisionMaster.ModuleMainWindow.ModuleDialogNew.DeepLearning.FlawPolygonRoiParameter>')
        self.itemsData.appendChild(object)

        levelNum = self.doc.createElement('_LevelNum')
        levelNum_text = self.doc.createTextNode(levelNum_str)
        levelNum.appendChild(levelNum_text)
        object.appendChild(levelNum)

        flags = self.doc.createElement('flags')
        flag_text = self.doc.createTextNode(flags_str)
        flags.appendChild(flag_text)
        object.appendChild(flags)

        colorValue = self.doc.createElement('_ColorValue')
        color_text = self.doc.createTextNode(colorValue_str)
        colorValue.appendChild(color_text)
        object.appendChild(colorValue)

        backgroundColor = self.doc.createElement('_BackgroundColor')
        backgroundColor_text = self.doc.createTextNode(backgroundColor_str)
        backgroundColor.appendChild(backgroundColor_text)
        object.appendChild(backgroundColor)

        polygonPoints = self.doc.createElement("_PolygonPoints")
        object.appendChild(polygonPoints)

        for i, (x_text_str,y_text_str) in enumerate(coord):
            polygonPoint = self.doc.createElement('HikPcUI.ImageView.PolygonPoint'.format(i))
            polygonPoints.appendChild(polygonPoint)

            x = self.doc.createElement('x')
            x_text = self.doc.createTextNode(str(x_text_str))
            x.appendChild(x_text)
            polygonPoint.appendChild(x)

            y = self.doc.createElement('y')
            y_text = self.doc.createTextNode(str(y_text_str))
            y.appendChild(y_text)
            polygonPoint.appendChild(y)

        sing = self.doc.createElement('_Sign')
        sing_text = self.doc.createTextNode(sing_str)
        sing.appendChild(sing_text)
        object.appendChild(sing)

        visible = self.doc.createElement('_TIsVisible')
        visible_text = self.doc.createTextNode(visible_str)
        visible.appendChild(visible_text)
        object.appendChild(visible)

        self.anno.appendChild(self.itemsData)

    def add_isOkCalibrated(self,calibrated="False"):
        calibrate = self.doc.createElement("_IsOKCalibrated")
        calibrated_text = self.doc.createTextNode(calibrated)
        calibrate.appendChild(calibrated_text)
        self.anno.appendChild(calibrate)

    def add_path(self, path_text_str="None"):
        path = self.doc.createElement('_ImagePath')
        path_text = self.doc.createTextNode(path_text_str)
        path.appendChild(path_text)
        self.anno.appendChild(path)

    def get_anno(self):
        return self.anno

    def get_doc(self):
        return self.doc

    def save_doc(self, save_path):
        with open(save_path, "w") as f:
            self.doc.writexml(f, indent='\t', newl='\n', addindent='\t', encoding='utf-8')

```

```python
import json
import os
from tqdm import tqdm

from read_json_anno import ReadAnno
from create_xml_anno import CreateAnno

backgroundColor=['#FFB6C1','#DC143C','#9370DB','#0000CD','#F0F8FF','#D4F2E7','#7FFFAA','#228B22','#8B0000']

def json_transform_xml(json_path, xml_path, json_label_path, process_mode="rectangle"):
    json_path = json_path
    print(json_path)
    json_anno = ReadAnno(json_path, process_mode=process_mode)
    width, height = json_anno.get_width_height()
    filename = json_anno.get_filename()
    coordis = json_anno.get_coordis()
    labelsDict = json.load(open(json_label_path))

    xml_anno = CreateAnno()

    for label,coord in zip(json_anno.labels,json_anno.coordis):
        #todo 根据lable获取label_idx
        if label in labelsDict.keys():
            label_idx = labelsDict[label]
            #修改标签
            if label == 'screen_normal':
                label = 'screenN'
            elif label == "screen_unnormal":
                label = 'screenU'
        else:
            label_idx = 8

        xml_anno.add_object(levelNum_str=str(label_idx),
                              flags_str=label,
                              colorValue_str=str(label_idx),
                              backgroundColor_str=backgroundColor[label_idx],
                              coord=coord,
                              sing_str='True',
                              visible_str='True')
    xml_anno.add_isOkCalibrated(calibrated='False')
    xml_anno.add_path(path_text_str=json_anno.json_data['imagePath'])
    xml_anno.save_doc(xml_path)


if __name__ == "__main__":
    root_json_dir = "../json2visionPng/Segmentation"  # json文件夹路径
    root_save_xml_dir = "xml_vision_mask"  # 转换后保存的xml文件夹路径

    for json_filename in tqdm(os.listdir(root_json_dir)):
        json_path = os.path.join(root_json_dir, json_filename)
        save_xml_path = os.path.join(root_save_xml_dir, json_filename.replace(".json", ".xml"))
        filepath, tmpfilename = os.path.split(json_filename)
        shotname, extension = os.path.splitext(tmpfilename)
        # img_path = shotname + ".jpg"
        json_label_path = 'pascal_visionSegmentation_classes.json'
        json_transform_xml(json_path, save_xml_path, json_label_path, process_mode="polygon")
        # json_transform_xml(json_path, save_xml_path, process_mode="polygon")

```



## coco数据解读



## pytorch数据读取实例

### voc语义分割数据读取

```python

def read_voc_images(root,is_trian=True,max_num=None):
    #读取voc的数据
    txt_fname = '%s/ImageSets/Segmentation/%s'%(root,'train.txt' if is_trian else 'val.txt')
    with open(txt_fname,'r') as f:
        images = f.read().split()
    if max_num is not None:
        images = images[:min(max_num, len(images))]
    features, labels = [None]*len(images),[None]*len(images)
    for i, fname in tqdm(enumerate(images)):
        features[i]=Image.open('%s/JPEGImages/%s.jpg'%(root,fname)).convert("RGB")
        labels[i]=Image.open('%s/SegmentationClass/%s.png'%(root,fname)).convert("RGB")
    return features,labels#PIL image

#自定义语义分割数据集类
class VOCSegDataset(Dataset):
    def __init__(self,is_train,crop_size,voc_dir,colormap2label=colormap2label,max_num=None):
        self.rgb_mean = np.array([0.485,0.465,0.406])
        self.rgb_std = np.array([0.229,0.224,0.225])
        self.tsf = transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize(mean=self.rgb_mean,
                                 std=self.rgb_std)
        ])
        self.crop_size = crop_size
        features, labels = read_voc_images(root=voc_dir,
                                           is_trian=is_train,
                                           max_num=max_num)
        self.features = self.filter(features) #PIL image
        self.labels = self.filter(labels)#PIL image
        self.colormap2label=colormap2label
        print('read '+str(len(self.features))+' valid example')

    def filter(self,imgs):
        return [img for img in imgs if (
            img.size[1]>=self.crop_size[0] and
            img.size[0]>self.crop_size[1]
        )]

    def __getitem__(self, idx):
        feature, label = voc_rand_crop(self.features[idx],
                                       self.labels[idx],
                                       *self.crop_size)
        return (self.tsf(feature),
                voc_label_indices(label,self.colormap2label))

    def __len__(self):
        return len(self.features)
```

segmentation_utils

```python
def voc_rand_crop(feature, label, height, width):
    '''
    random crop feature (PIL Image) and label (PIL Image)
    :param feature:图像
    :param label:图像label
    :param height:裁剪的高
    :param width:裁剪的宽
    :return:
    '''
    i,j,h,w = torchvision.transforms.RandomCrop.get_params(
        feature,output_size=(height,width)
    )
    feature = torchvision.transforms.functional.crop(feature, i, j, h, w)
    label = torchvision.transforms.functional.crop(label, i, j, h, w)
    return feature,label

#segmentation数据处理
VOC_COLORMAP = [[0, 0, 0], [128, 0, 0], [0, 128, 0], [128, 128, 0],
                [0, 0, 128], [128, 0, 128], [0, 128, 128], [128, 128, 128],
                [64, 0, 0], [192, 0, 0], [64, 128, 0], [192, 128, 0],
                [64, 0, 128], [192, 0, 128], [64, 128, 128], [192, 128, 128],
                [0, 64, 0], [128, 64, 0], [0, 192, 0], [128, 192, 0],
                [0, 64, 128]]

VOC_CLASSES = ['background', 'aeroplane', 'bicycle', 'bird', 'boat',
               'bottle', 'bus', 'car', 'cat', 'chair', 'cow',
               'diningtable', 'dog', 'horse', 'motorbike', 'person',
               'potted plant', 'sheep', 'sofa', 'train', 'tv/monitor']
def label2image(pred):
    #pred: [320,480]
    colormap = torch.tensor(VOC_COLORMAP, device=device,dtype=int)
    x = pred.long()
    imgs = (colormap[x,:]).data.cpu().numpy()
    return imgs

#定义两个常量,查找标签中每个像素的类别索引
colormap2label = torch.zeros(256**3,dtype=torch.uint8)
for i, colormap in enumerate(VOC_COLORMAP):
    colormap2label[(colormap[0]*256+colormap[1])*256+colormap[2]]=i
def voc_label_indices(colormap, colormap2label):
    '''
    convert colormap (PIL image) to colormap2label (uint8 tensor)
    :param colormap: seg image
    :param colormap2label: int
    :return:
    '''
    if isinstance(colormap,Tensor):
        colormap = colormap.permute(1,2,0)
        colormap = colormap.numpy()
    colormap = np.array(colormap)
    colormap = colormap.astype("int32")
    idx = ((colormap[:,:,0]*256+colormap[:,:,1])*256+colormap[:,:,2])
    return colormap2label[idx]
```

### VOC目标检测数据读取

```python

class VOCDataSet(Dataset):
    """读取解析PASCAL VOC2007/2012数据集"""
    def __init__(self, voc_root, name='2012',transforms=None,
                 txt_name:str="train.txt",max_num=None):
        self.root = os.path.join(voc_root,name)
        self.img_root = os.path.join(self.root,"JPEGImages")
        self.mask_root = os.path.join(self.root,"SegmentationClassPNG")
        self.annotations_root = os.path.join(self.root,"Annotations")

        #read train.txt or val.txt
        txt_path = os.path.join(self.root,"ImageSets","Main",txt_name)#todo 读取训练文件
        assert os.path.exists(txt_path), "not found {} file".format(txt_name)

        with open(txt_path) as read:
            self.xml_list = [os.path.join(self.annotations_root,line.strip()+".xml")
                             for line in read.readlines() if len(line.strip())>0]

        #check file
        assert len(self.xml_list)>0, "in {} file does not find any information".format(txt_path)
        for xml_path in self.xml_list:
            assert os.path.exists(xml_path), f"{xml_path} file not exist."
        if max_num is not None:
            max_num = min(max_num,len(self.xml_list))
            self.xml_list = self.xml_list[:max_num]

        json_file = "./pascal_voc_classes.json"
        assert os.path.exists(json_file), f"{json_file} file not exist."

        #read class_indict
        json_file = open(json_file,'r')
        self.class_dict = json.load(json_file)
        json_file.close()

        self.transforms = transforms

    def __len__(self):
        return len(self.xml_list)

    def __getitem__(self, idx):
        #read xml
        xml_path = self.xml_list[idx]
        with open(xml_path) as fid:
            xml_str = fid.read()
        xml = etree.fromstring(xml_str)#解析成ElementTree
        data = self.parse_xml_to_dict(xml)["annotation"]
        img_path = os.path.join(self.img_root,data['filename'])
        image = Image.open(img_path).convert("RGB")
        mask_path = os.path.join(self.mask_root,data['filename'].split(".")[0]+'.png')

        # if image.format != "RGB":
        #     raise ValueError(f"Image '{img_path}' format not JEPG.")

        #判断路径是否存在
        if os.path.exists(mask_path):
            mask = Image.open(mask_path).convert("RGB")
            # mask = Image.open(mask_path)
            # plt.imshow(mask)
            # plt.show()
        else:
            mask = np.zeros_like(image)
            shape = mask.shape
            mask = np.random.randint(255,size=shape,dtype="uint8")
            mask = Image.fromarray(mask)

        boxes = []
        labels =[]
        iscrowd = []
        assert "object" in data, f"{xml_path} lack of object information."
        for obj in data["object"]:
            xmin = float(obj['bndbox']['xmin'])
            xmax = float(obj['bndbox']['xmax'])
            ymin = float(obj['bndbox']['ymin'])
            ymax = float(obj['bndbox']['ymax'])
            #进一步检查数据, 有的标注信息中可能有w或h为0的情况, 这样的数据会导致计算回归loss为nan
            if xmax <= xmin or ymax<=ymin:
                print(f"waring:in {xml_path} xml, there some bbox w/h <=0")
                continue
            boxes.append([xmin,ymin,xmax,ymax])
            labels.append(self.class_dict[obj["name"]])
            if "difficult" in obj:
                iscrowd.append(int(obj["difficult"]))
            else:
                iscrowd.append(0)

        #convert everything into a torch.Tensor
        boxes = torch.as_tensor(boxes,dtype=torch.float32)
        labels = torch.as_tensor(labels,dtype=torch.int64)
        iscrowd = torch.as_tensor(iscrowd,dtype=torch.int64)
        image_id = torch.tensor([idx])
        area = (boxes[:,3]-boxes[:,1])*(boxes[:,2]-boxes[:,0])

        target = {}
        target["boxes"]=boxes
        target["labels"]=labels
        target["image_id"]=image_id
        target["area"]=area
        target["iscrowd"]=iscrowd

        if self.transforms is not None:
            image, target,mask = self.transforms(image, target,mask)
            # mask进行转换:先变为tensor,再转为PIL
            # mask = t.ToPILImage(mask)
        #再用voc_label_indices标为label
        mask = voc_label_indices(mask, colormap2label)
        target["masks"]=mask
        return image,target

    def get_height_and_width(self,idx):
        #read xml
        xml_path = self.xml_list[idx]
        with open(xml_path) as fid:
            xml_str = fid.read()
        xml = etree.fromstring(xml_str)
        data = self.parse_xml_to_dict(xml)["annotation"]
        data_height = int(data['size']['height'])
        data_width = int(data['size']['width'])
        return data_height, data_width

    def coco_index(self,idx):
        """
        该方法时专门为pycocotools统计标签信息准备, 不对图像和标签作任何处理,由于不用去
        读取图片,可大幅度缩减统计时间
        :param idx: 输入需要获取图像的索引
        :return:
        """
        #read xml
        xml_path = self.xml_list[idx]
        with open(xml_path) as fid:
            xml_str = fid.read()
        xml = etree.fromstring(xml_str)
        data = self.parse_xml_to_dict(xml)["annotation"]
        data_height = int(data["size"]["height"])
        data_width = int(data["size"]["width"])
        boxes = []
        labels = []
        iscrowd = []
        for obj in data["object"]:
            xmin = float(obj["bndbox"]["xmin"])
            xmax = float(obj["bndbox"]["xmax"])
            ymin = float(obj["bndbox"]["ymin"])
            ymax = float(obj["bndbox"]["ymax"])
            boxes.append([xmin, ymin, xmax, ymax])
            labels.append(self.class_dict[obj["name"]])
            iscrowd.append(int(obj["difficult"]))

        # convert everything into a torch.Tensor
        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        labels = torch.as_tensor(labels, dtype=torch.int64)
        iscrowd = torch.as_tensor(iscrowd, dtype=torch.int64)
        image_id = torch.tensor([idx])
        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])

        target = {}
        target["boxes"] = boxes
        target["labels"] = labels
        target["image_id"] = image_id
        target["area"] = area
        target["iscrowd"] = iscrowd
        return (data_height,data_width),target

    def parse_xml_to_dict(self,xml):
        """
        将xml文件解析成字典形式.
        :param xml: xml tree Obtained by parsing xml file contents using lxml.etree
        :return: python dictionary holding xml contents
        """
        if len(xml)==0:#遍历到底底层, 直接返回tag对象的信息
            return {xml.tag: xml.text}

        result = {}
        for child in xml:
            child_result = self.parse_xml_to_dict(child)#递归遍历标签信息
            if child.tag != "object":
                result[child.tag]=child_result[child.tag]
            else:
                if child.tag not in result:#因为object可能有多个, 所以需要放入列表里
                    result[child.tag] = []
                result[child.tag].append(child_result[child.tag])
        return {xml.tag:result}

    @staticmethod
    def collate_fn(batch):
        return tuple(zip(*batch))
```

### COCO目标检测数据读取

```python

def _coco_remove_images_without_annotations(dataset, ids):
    """
    删除coco数据集中没有目标，或者目标面积非常小的数据
    refer to:
    https://github.com/pytorch/vision/blob/master/references/detection/coco_utils.py
    :param dataset:
    :param cat_list:
    :return:
    """
    def _has_only_empty_bbox(anno):
        return all(any(o <= 1 for o in obj["bbox"][2:]) for obj in anno)

    def _has_valid_annotation(anno):
        # if it's empty, there is no annotation
        if len(anno) == 0:
            return False
        # if all boxes have close to zero area, there is no annotation
        if _has_only_empty_bbox(anno):
            return False

        return True

    valid_ids = []
    for ds_idx, img_id in enumerate(ids):
        ann_ids = dataset.getAnnIds(imgIds=img_id, iscrowd=None)
        anno = dataset.loadAnns(ann_ids)

        if _has_valid_annotation(anno):
            valid_ids.append(img_id)

    return valid_ids


class CocoDetection(data.Dataset):
    """`MS Coco Detection <https://cocodataset.org/>`_ Dataset.

    Args:
        root (string): Root directory where images are downloaded to.
        annFile (string): Path to json annotation file.
        transforms (callable, optional): A function/transform that takes input sample and its target as entry
            and returns a transformed version.
    """

    def __init__(self, root, dataset="train", transforms=None):
        super(CocoDetection, self).__init__()
        assert dataset in ["val"], 'dataset must be in ["train", "val"]'
        anno_file = "instances_{}2014.json".format(dataset)
        assert os.path.exists(root), "file '{}' does not exist.".format(root)
        self.img_root = os.path.join(root, "{}2014".format(dataset))
        assert os.path.exists(self.img_root), "path '{}' does not exist.".format(self.img_root)
        self.anno_path = os.path.join(root, "annotations", anno_file)
        assert os.path.exists(self.anno_path), "file '{}' does not exist.".format(self.anno_path)

        self.mode = dataset
        self.transforms = transforms
        self.coco = COCO(self.anno_path)

        if dataset == "train":
            # 获取coco数据索引与类别名称的关系
            # 注意在object80中的索引并不是连续的，虽然只有80个类别，但索引还是按照stuff91来排序的
            coco_classes = dict([(v["id"], v["name"]) for k, v in self.coco.cats.items()])

            # 将stuff91的类别索引重新编排，从1到80
            coco91to80 = dict([(str(k), idx+1) for idx, (k, _) in enumerate(coco_classes.items())])
            json_str = json.dumps(coco91to80, indent=4)
            with open('coco91_to_80.json', 'w') as json_file:
                json_file.write(json_str)

            # 记录重新编排后的索引以及类别名称关系
            coco80_info = dict([(str(idx+1), v) for idx, (_, v) in enumerate(coco_classes.items())])
            json_str = json.dumps(coco80_info, indent=4)
            with open('coco80_indices.json', 'w') as json_file:
                json_file.write(json_str)
        else:
            # 如果是验证集就直接读取生成好的数据
            coco91to80_path = 'coco91_to_80.json'
            assert os.path.exists(coco91to80_path), "file '{}' does not exist.".format(coco91to80_path)

            coco91to80 = json.load(open(coco91to80_path, "r"))

        self.coco91to80 = coco91to80

        ids = list(sorted(self.coco.imgs.keys()))
        if dataset == "train":
            # 移除没有目标，或者目标面积非常小的数据
            valid_ids = _coco_remove_images_without_annotations(self.coco, ids)
            self.ids = valid_ids
        else:
            self.ids = ids

    def parse_targets(self,
                      img_id: int,
                      coco_targets: list,
                      w: int = None,
                      h: int = None):
        # 只筛选出单个对象的情况
        anno = [obj for obj in coco_targets if obj['iscrowd'] == 0]

        # 进一步检查数据，有的标注信息中可能有w或h为0的情况，这样的数据会导致计算回归loss为nan
        boxes = []
        for obj in anno:
            if obj["bbox"][2] > 0 and obj["bbox"][3] > 0:
                boxes.append(obj["bbox"])

        # guard against no boxes via resizing
        boxes = torch.as_tensor(boxes, dtype=torch.float32).reshape(-1, 4)
        # [xmin, ymin, w, h] -> [xmin, ymin, xmax, ymax]
        boxes[:, 2:] += boxes[:, :2]
        if (w is not None) and (h is not None):
            boxes[:, 0::2].clamp_(min=0, max=w)
            boxes[:, 1::2].clamp_(min=0, max=h)

        classes = [self.coco91to80[str(obj["category_id"])] for obj in anno]
        classes = torch.tensor(classes, dtype=torch.int64)

        target = {}
        target["boxes"] = boxes
        target["labels"] = classes
        target["image_id"] = torch.tensor([img_id])

        # for conversion to coco api
        area = torch.tensor([obj["area"] for obj in anno])
        iscrowd = torch.tensor([obj["iscrowd"] for obj in anno])
        target["area"] = area
        target["iscrowd"] = iscrowd

        return target

    def __getitem__(self, index):
        """
        Args:
            index (int): Index

        Returns:
            tuple: Tuple (image, target). target is the object returned by ``coco.loadAnns``.
        """
        coco = self.coco
        img_id = self.ids[index]
        ann_ids = coco.getAnnIds(imgIds=img_id)
        coco_target = coco.loadAnns(ann_ids)

        path = coco.loadImgs(img_id)[0]['file_name']
        img = Image.open(os.path.join(self.img_root, path)).convert('RGB')

        w, h = img.size
        target = self.parse_targets(img_id, coco_target, w, h)
        if self.transforms is not None:
            img, target = self.transforms(img, target)

        return img, target

    def __len__(self):
        return len(self.ids)

    def get_height_and_width(self, index):
        coco = self.coco
        img_id = self.ids[index]

        img_info = coco.loadImgs(img_id)[0]
        w = img_info["width"]
        h = img_info["height"]
        return h, w

    @staticmethod
    def collate_fn(batch):
        return tuple(zip(*batch))
```

